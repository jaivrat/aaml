{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb8c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4295bb",
   "metadata": {},
   "source": [
    "Accuracy Score = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1: It is defined as a simple weighted average (harmonic mean) of precision and recall.\n",
    "\n",
    "F1 = 2PR / (P + R)\n",
    "\n",
    "(Note that HM is close wo lower side of two numbers, not like AM which is in between)\n",
    "\n",
    "I better remeber these (as i use in ROC/AUC):\n",
    "\n",
    "*  ***TPR*** or True Positive Rate. Also called **Senstivity** = how many positive records correctly predicted? = TP/(TP+FN)\n",
    "\n",
    "* ***FPR*** or False Positive Rate. ***Specificity*** = 1 - FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d0f404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "metrics.accuracy_score(l1, l2)\n",
    "metrics.f1_score(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ce724",
   "metadata": {},
   "source": [
    "AUC:\n",
    "\n",
    "* AUC = 1 => Perfect model\n",
    "\n",
    "* AUC = 0.5 => Random guess ..\n",
    "\n",
    "* AUC < 0.5 => Not possible. Just invert prediction - may be making some mistake\n",
    "\n",
    "LOG LOSS Metrics:\n",
    "\n",
    "Log Loss = - 1.0 * ( target * log(prediction) + (1 - target) * log(1 - prediction) )\n",
    "\n",
    "For multiple samples, obviously take average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3088a32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49882711861432294"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_loss(y_true, y_proba):\n",
    "    \"\"\"\n",
    "    Function to calculate log loss\n",
    "    :param y_true: list of true values\n",
    "    :param y_proba: list of probabilities for 1\n",
    "    :return: overall log loss\n",
    "    \"\"\"\n",
    "    # define an epsilon value\n",
    "    # this can also be an input\n",
    "    # this value is used to clip probabilities\n",
    "    epsilon = 1e-15\n",
    "    # initialize empty list to store\n",
    "    # individual losses\n",
    "    loss = []\n",
    "    # loop over all true and predicted probability values\n",
    "    for yt, yp in zip(y_true, y_proba):\n",
    "        # adjust probability\n",
    "        # 0 gets converted to 1e-15\n",
    "        # 1 gets converted to 1 - 1e-15\n",
    "        # Why? Think about it! => Numerical errors for log functions\n",
    "        yp = np.clip(yp, epsilon, 1 - epsilon)\n",
    "        # calculate loss for one sample\n",
    "        temp_loss = - 1.0 * (yt * np.log(yp) + (1 - yt) * np.log(1 - yp))\n",
    "        # add to loss list\n",
    "        loss.append(temp_loss)\n",
    "    # return mean loss over all samples\n",
    "    return np.mean(loss)\n",
    "\n",
    "\n",
    "y_true =  [0,     0,   0,   0,   1,    0,   1,   0,   0,    1,   0,   1,    0,    0,    1]\n",
    "y_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05, 0.9, 0.5, 0.3, 0.66, 0.3, 0.2, 0.85, 0.15, 0.99]\n",
    "metrics.log_loss(y_true, y_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c8bfed",
   "metadata": {},
   "source": [
    "For multi-class classification, if we need to calc precision etc, \n",
    "\n",
    "\n",
    "**Macro averaged precision:** \n",
    "calculate precision for all classes individually and then average them\n",
    "\n",
    "**Micro averaged precision:**\n",
    "calculate class wise true positive and false positive and then use that to calculate overall precision\n",
    "\n",
    "**Weighted precision:**\n",
    "same as macro but in this case, it is weighted average depending on the number of items in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03208127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate macro averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: macro precision score\n",
    "    \"\"\"\n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate true positive for current class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate false positive for current class\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate precision for current class\n",
    "        temp_precision = tp / (tp + fp)\n",
    "        \n",
    "        # keep adding precision for all classes\n",
    "        precision += temp_precision\n",
    "        \n",
    "    # calculate and return average precision over all classes\n",
    "    precision /= num_classes\n",
    "    return precision\n",
    "\n",
    "\n",
    "def micro_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate micro averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: micro precision score\n",
    "    \"\"\"\n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    # initialize tp and fp to 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        # calculate true positive for current class\n",
    "        # and update overall tp\n",
    "        tp += true_positive(temp_true, temp_pred)\n",
    "        # calculate false positive for current class\n",
    "        # and update overall tp\n",
    "        fp += false_positive(temp_true, temp_pred)\n",
    "    # calculate and return overall precision\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def weighted_precision(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate weighted averaged precision\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: weighted precision score\n",
    "    \"\"\"\n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    \n",
    "    # create class:sample count dictionary\n",
    "    # it looks something like this:\n",
    "    # {0: 20, 1:15, 2:21}\n",
    "    class_counts = Counter(y_true)\n",
    "    \n",
    "    # initialize precision to 0\n",
    "    precision = 0\n",
    "    \n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        \n",
    "        # calculate tp and fp for class\n",
    "        tp = true_positive(temp_true, temp_pred)\n",
    "        fp = false_positive(temp_true, temp_pred)\n",
    "        \n",
    "        # calculate precision of class\n",
    "        temp_precision = tp / (tp + fp)\n",
    "        \n",
    "        # multiply precision with count of samples in class\n",
    "        weighted_precision = class_counts[class_] * temp_precision\n",
    "        \n",
    "        # add to overall precision\n",
    "        precision += weighted_precision\n",
    "        \n",
    "    # calculate overall precision by dividing by\n",
    "    # total number of samples\n",
    "    overall_precision = precision / len(y_true)\n",
    "        \n",
    "    return overall_precision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b983e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_f1(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate weighted f1 score\n",
    "    :param y_true: list of true values\n",
    "    :param y_proba: list of predicted values\n",
    "    :return: weighted f1 score\n",
    "    \"\"\"\n",
    "    # find the number of classes by taking\n",
    "    # length of unique values in true list\n",
    "    num_classes = len(np.unique(y_true))\n",
    "    # create class:sample count dictionary\n",
    "    # it looks something like this:\n",
    "    # {0: 20, 1:15, 2:21}\n",
    "    class_counts = Counter(y_true)\n",
    "    # initialize f1 to 0\n",
    "    f1 = 0\n",
    "    # loop over all classes\n",
    "    for class_ in range(num_classes):\n",
    "        # all classes except current are considered negative\n",
    "        temp_true = [1 if p == class_ else 0 for p in y_true]\n",
    "        temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
    "        # calculate precision and recall for class\n",
    "        p = metrics.precision_score(temp_true, temp_pred)\n",
    "        r = metrics.recall_score(temp_true, temp_pred)\n",
    "        # calculate f1 of class\n",
    "        if p + r != 0:\n",
    "            temp_f1 = 2 * p * r / (p + r)\n",
    "        else:\n",
    "            temp_f1 = 0\n",
    "        # multiply f1 with count of samples in class\n",
    "        weighted_f1 = class_counts[class_] * temp_f1\n",
    "        # add to f1 precision\n",
    "        f1 += weighted_f1\n",
    "    # calculate overall F1 by dividing by\n",
    "    # total number of samples\n",
    "    overall_f1 = f1 / len(y_true)\n",
    "    return overall_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d23baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41269841269841273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41269841269841273"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
    "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
    "print(weighted_f1(y_true, y_pred))\n",
    "metrics.f1_score(y_true, y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "122d80fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    This function calculates r-squared score\n",
    "    :param y_true: list of real numbers, true values\n",
    "    :param y_pred: list of real numbers, predicted values\n",
    "    :return: r2 score\n",
    "    \"\"\"\n",
    "    # calculate the mean value of true values\n",
    "    mean_true_value = np.mean(y_true)\n",
    "    # initialize numerator with 0\n",
    "    numerator = 0\n",
    "    # initialize denominator with 0\n",
    "    denominator = 0\n",
    "    # loop over all true and predicted values\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        # update numerator\n",
    "        numerator += (yt - yp) ** 2\n",
    "        # update denominator\n",
    "        denominator += (yt - mean_true_value) ** 2\n",
    "    # calculate the ratio\n",
    "    ratio = numerator / denominator\n",
    "    # return 1 - ratio\n",
    "    return 1-ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd49ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaml",
   "language": "python",
   "name": "aaml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
